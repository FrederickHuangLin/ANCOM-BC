---
title: "Figure 3b-c, S1b-c, S2b-c"
author: "Huang Lin"
date: '`r format(Sys.Date(), "%B %d, %Y")`'
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
rm(list = ls())

library(tidyverse)
library(nloptr)
library(phyloseq)
library(ggpubr)
library(pander)
panderOptions('table.caption.prefix', NULL)
panderOptions('table.continues', NULL)
panderOptions('table.emphasize.rownames', FALSE)
```

# 1. Generate Simulation Data

## 1.1 Large variable sampling fractions

Unbalanced microbial loads and balanced library sizes

```{r, message=FALSE, warning=FALSE, comment=NA}
abn.tab.gen1=function(n.taxa, n.samp.grp1, n.samp.grp2, prop.diff, abn.seed, obs.seed, 
                      struc.zero.prop, out.zero.prop, library.size){
  # Total number of samples
  n.samp=n.samp.grp1+n.samp.grp2
  
  set.seed(abn.seed) # This seed is used to control whether you would like to have the same population
  low.prop=0.6 # Proportion of low abundance 
  med.prop=0.3 # Proportion of medium abundance
  hi.prop=0.1  # Proportion of high abundance
  # Indices for taxa abundance 
  index=sample(c(1, 2, 3), n.taxa, replace = T, prob = c(low.prop, med.prop, hi.prop)) 
  
  # Poisson parameters
  lambda=rep(NA, n.taxa)
  lambda[which(index==1)]=rgamma(length(which(index==1)), shape=50, rate=1)
  lambda[which(index==2)]=rgamma(length(which(index==2)), shape=200, rate=1)
  lambda[which(index==3)]=rgamma(length(which(index==3)), shape=10000, rate=1)
  
  # Which taxa are differentially abundant
  diff.ind=rep(0, n.taxa)
  # Group1 is higher than group2
  diff1.ind=sample(c(1:n.taxa), floor(n.taxa*prop.diff), replace=FALSE)
  diff.ind[diff1.ind]=1
  # Group2 is higher than group1
  wt=runif(1, 0, 1)
  diff2.ind=sample(diff1.ind, wt*length(diff1.ind), replace=FALSE)
  diff.ind[diff2.ind]=2
  # Structural zeros
  diff3.ind=sample(which(diff.ind==0), struc.zero.prop*length(which(diff.ind==0)), replace = FALSE)
  diff.ind[diff3.ind]=-1
  
  # Signal size
  effect.size=rep(1, n.taxa)
  effect.size[diff1.ind]=runif(length(diff1.ind), 1, 10)
  effect.size[diff2.ind]=runif(length(diff2.ind), 0.1, 1)
  effect.size[diff3.ind]=0
  names(effect.size)=paste0("taxon", seq(n.taxa))
  
  # Abundance template
  temp.grp1=round(lambda*effect.size)
  temp.grp2=round(lambda)
  for (i in which(effect.size!=1)) {
    if(temp.grp1[i]==temp.grp2[i]) temp.grp1[i]=temp.grp1[i]+1
  }
  temp.dat=data.frame(temp.grp1, temp.grp2, effect.size)
  rownames(temp.dat)=paste0("taxon", seq(n.taxa))
  
  # Abundance table in the ecosystem
  abn.mat=matrix(0, ncol=n.samp, nrow=n.taxa)
  for(i in 1:n.taxa){
    abn.mat[i, ]=c(rpois(n.samp.grp1, temp.grp1[i]), rpois(n.samp.grp2, temp.grp2[i]))
  }
  # Outlier zeros
  out.ind=rep(0, n.taxa); out.ind[sample(seq(n.taxa), out.zero.prop*n.taxa, replace = F)]=1
  names(out.ind)=paste0("taxon", seq(n.taxa))
  abn.mat[which(out.ind==1), sample(seq(n.samp), out.zero.prop*n.samp, replace = F)]=0
  
  abn.total=colSums(abn.mat)
  names(abn.total)=paste0("sub", seq(n.samp))
  
  # Number of taxa that are sampled for each subject
  if(library.size=="large"){
    depth=1/runif(n.samp, 5, 10)
    obs.total=round(max(abn.total)*depth)
  }else if(library.size=="small"){
    depth=1/sample(c(runif(n.samp, 10, 50), runif(n.samp, 100, 500)), n.samp, replace = T)
    obs.total=round(max(abn.total)*depth)
  }
  names(obs.total)=paste0("sub", seq(n.samp))
  
  # Specimen abundance
  set.seed(obs.seed)
  obs.list=lapply(1:n.samp, function(i) 
    phyloseq:::rarefaction_subsample(x=abn.mat[, i], sample.size=obs.total[i]))
  obs.mat=Reduce('cbind', obs.list)
  
  # Prepare output data sets
  abn.dat=data.frame(abn.mat, row.names = NULL)
  rownames(abn.dat)=paste0("taxon", seq(n.taxa))
  colnames(abn.dat)=paste0("sub", seq(n.samp))
  
  obs.dat=data.frame(obs.mat, row.names = NULL)
  rownames(obs.dat)=paste0("taxon", seq(n.taxa))
  colnames(obs.dat)=paste0("sub", seq(n.samp))
  
  grp.ind=c(rep(1, n.samp.grp1), rep(2, n.samp.grp2))
  names(grp.ind)=paste0("sub", seq(n.samp))
  
  names(diff.ind)=paste0("taxon", seq(n.taxa))
  
  c.mult=obs.total/abn.total
  names(c.mult)=paste0("sub", seq(n.samp))
  
  test.data=list(temp.dat, abn.dat, obs.dat, effect.size, grp.ind, 
                 diff.ind, out.ind, c.mult, abn.total, obs.total)
  names(test.data)=c("template", "pop.abn", "obs.abn", "effect.size", "grp", 
                     "diff.taxa", "outlier", "mult", "abn.total", "obs.total")
  return(test.data)
}
```

## 1.2 Moderate variable sampling fractions

Unbalanced microbial loads and unbalanced library sizes

```{r, message=FALSE, warning=FALSE, comment=NA}
abn.tab.gen2=function(n.taxa, n.samp.grp1, n.samp.grp2, prop.diff, abn.seed, obs.seed,
                      struc.zero.prop, out.zero.prop, library.size){
  # Total number of samples
  n.samp=n.samp.grp1+n.samp.grp2
  
  set.seed(abn.seed) # This seed is used to control whether you would like to have the same population
  low.prop=0.6 # Proportion of low abundance 
  med.prop=0.3 # Proportion of medium abundance
  hi.prop=0.1  # Proportion of high abundance
  # Indices for taxa abundance 
  index=sample(c(1, 2, 3), n.taxa, replace = T, prob = c(low.prop, med.prop, hi.prop)) 
  
  # Poisson parameters
  lambda=rep(NA, n.taxa)
  lambda[which(index==1)]=rgamma(length(which(index==1)), shape=50, rate=1)
  lambda[which(index==2)]=rgamma(length(which(index==2)), shape=200, rate=1)
  lambda[which(index==3)]=rgamma(length(which(index==3)), shape=10000, rate=1)
  
  # Which taxa are differentially abundant
  diff.ind=rep(0, n.taxa)
  # Group1 is higher than group2
  diff1.ind=sample(c(1:n.taxa), floor(n.taxa*prop.diff), replace=FALSE)
  diff.ind[diff1.ind]=1
  # Group2 is higher than group1
  wt=runif(1, 0, 1)
  diff2.ind=sample(diff1.ind, wt*length(diff1.ind), replace=FALSE)
  diff.ind[diff2.ind]=2
  # Structural zeros
  diff3.ind=sample(which(diff.ind==0), struc.zero.prop*length(which(diff.ind==0)), replace = FALSE)
  diff.ind[diff3.ind]=-1
  
  # Signal size
  effect.size=rep(1, n.taxa)
  effect.size[diff1.ind]=runif(length(diff1.ind), 1, 10)
  effect.size[diff2.ind]=runif(length(diff2.ind), 0.1, 1)
  effect.size[diff3.ind]=0
  names(effect.size)=paste0("taxon", seq(n.taxa))
  
  # Abundance template
  temp.grp1=round(lambda*effect.size)
  temp.grp2=round(lambda)
  for (i in which(effect.size!=1)) {
    if(temp.grp1[i]==temp.grp2[i]) temp.grp1[i]=temp.grp1[i]+1
  }
  temp.dat=data.frame(temp.grp1, temp.grp2, effect.size)
  rownames(temp.dat)=paste0("taxon", seq(n.taxa))
  
  # Abundance table in the ecosystem
  abn.mat=matrix(0, ncol=n.samp, nrow=n.taxa)
  for(i in 1:n.taxa){
    abn.mat[i, ]=c(rpois(n.samp.grp1, temp.grp1[i]), rpois(n.samp.grp2, temp.grp2[i]))
  }
  # Outlier zeros
  out.ind=rep(0, n.taxa); out.ind[sample(seq(n.taxa), out.zero.prop*n.taxa, replace = F)]=1
  names(out.ind)=paste0("taxon", seq(n.taxa))
  abn.mat[which(out.ind==1), sample(seq(n.samp), out.zero.prop*n.samp, replace = F)]=0
  
  abn.total=colSums(abn.mat)
  names(abn.total)=paste0("sub", seq(n.samp))
  
  # Number of taxa that are sampled for each subject
  if(library.size=="large"){
    depth=1/runif(n.samp, 5, 10)
    obs.total=round(max(abn.total)*depth)
  }else if(library.size=="small"){
    depth=1/sample(c(runif(n.samp, 10, 50), runif(n.samp, 100, 500)), n.samp, replace = T)
    obs.total=round(abn.total*depth)
  }
  names(obs.total)=paste0("sub", seq(n.samp))
  
  # Specimen abundance
  set.seed(obs.seed)
  obs.list=lapply(1:n.samp, function(i) 
    phyloseq:::rarefaction_subsample(x=abn.mat[, i], sample.size=obs.total[i]))
  obs.mat=Reduce('cbind', obs.list)
  
  # Prepare output data sets
  abn.dat=data.frame(abn.mat, row.names = NULL)
  rownames(abn.dat)=paste0("taxon", seq(n.taxa))
  colnames(abn.dat)=paste0("sub", seq(n.samp))
  
  obs.dat=data.frame(obs.mat, row.names = NULL)
  rownames(obs.dat)=paste0("taxon", seq(n.taxa))
  colnames(obs.dat)=paste0("sub", seq(n.samp))
  
  grp.ind=c(rep(1, n.samp.grp1), rep(2, n.samp.grp2))
  names(grp.ind)=paste0("sub", seq(n.samp))
  
  names(diff.ind)=paste0("taxon", seq(n.taxa))
  
  c.mult=obs.total/abn.total
  names(c.mult)=paste0("sub", seq(n.samp))
  
  test.data=list(temp.dat, abn.dat, obs.dat, effect.size, grp.ind, 
                 diff.ind, out.ind, c.mult, abn.total, obs.total)
  names(test.data)=c("template", "pop.abn", "obs.abn", "effect.size", "grp", 
                     "diff.taxa", "outlier", "mult", "abn.total", "obs.total")
  return(test.data)
}
```

## 1.3 Small variable sampling fractions

Balanced microbial loads and balanced library sizes

```{r, message=FALSE, warning=FALSE, comment=NA}
abn.tab.gen3=function(n.taxa, n.samp.grp1, n.samp.grp2, prop.diff, abn.seed, obs.seed,
                      struc.zero.prop, out.zero.prop, library.size){
  # Total number of samples
  n.samp=n.samp.grp1+n.samp.grp2
  
  set.seed(abn.seed) # This seed is used to control whether you would like to have the same population
  low.prop=0.6 # Proportion of low abundance 
  med.prop=0.3 # Proportion of medium abundance
  hi.prop=0.1  # Proportion of high abundance
  # Indices for taxa abundance 
  index=sample(c(1, 2, 3), n.taxa, replace = T, prob = c(low.prop, med.prop, hi.prop)) 
  
  # Poisson parameters
  lambda=rep(NA, n.taxa)
  lambda[which(index==1)]=rgamma(length(which(index==1)), shape=50, rate=1)
  lambda[which(index==2)]=rgamma(length(which(index==2)), shape=200, rate=1)
  lambda[which(index==3)]=rgamma(length(which(index==3)), shape=10000, rate=1)
  
  # Which taxa are differentially abundant
  diff.ind=rep(0, n.taxa)
  # Differentially abundant taxa
  diff.pos=sample(c(1:n.taxa), floor(n.taxa*prop.diff), replace=FALSE)
  diff.ind[diff.pos]=1
  # Structural zeros
  szero.pos=sample(which(diff.ind==0), struc.zero.prop*length(which(diff.ind==0)), replace = FALSE)
  diff.ind[szero.pos]=-1
  
  # Signal size
  effect.size=rep(1, n.taxa)
  effect.size[diff.pos]=runif(length(diff.pos), 1, 10)
  effect.size[szero.pos]=0
  names(effect.size)=paste0("taxon", seq(n.taxa))
  
  # Abundance template
  temp.grp1=round(lambda); temp.grp2=round(lambda)
  for (i in diff.pos) {
    rand.ind=sample(c(1, 2), 1)
    if (rand.ind==1){
      temp.grp1[i]=temp.grp1[i]*effect.size[i]
    }else{
      temp.grp2[i]=temp.grp2[i]*effect.size[i]
    }
  }
  for (i in szero.pos) {
    rand.ind=sample(c(1, 2), 1)
    if (rand.ind==1){
      temp.grp1[i]=temp.grp1[i]*effect.size[i]
    }else{
      temp.grp2[i]=temp.grp2[i]*effect.size[i]
    }
  }
  temp.dat=data.frame(temp.grp1, temp.grp2, effect.size)
  rownames(temp.dat)=paste0("taxon", seq(n.taxa))
  
  # Abundance table in the ecosystem
  abn.mat=matrix(0, ncol=n.samp, nrow=n.taxa)
  for(i in 1:n.taxa){
    abn.mat[i, ]=c(rpois(n.samp.grp1, temp.grp1[i]), rpois(n.samp.grp2, temp.grp2[i]))
  }
  # Outlier zeros
  out.ind=rep(0, n.taxa); out.ind[sample(seq(n.taxa), out.zero.prop*n.taxa, replace = F)]=1
  names(out.ind)=paste0("taxon", seq(n.taxa))
  abn.mat[which(out.ind==1), sample(seq(n.samp), out.zero.prop*n.samp, replace = F)]=0
  
  abn.total=colSums(abn.mat)
  names(abn.total)=paste0("sub", seq(n.samp))
  
  # Number of taxa that are sampled for each subject
  if(library.size=="large"){
    depth=1/runif(n.samp, 5, 10)
    obs.total=round(max(abn.total)*depth)
  }else if(library.size=="small"){
    depth=1/sample(c(runif(n.samp, 10, 50), runif(n.samp, 100, 500)), n.samp, replace = T)
    obs.total=round(max(abn.total)*depth)
  }
  names(obs.total)=paste0("sub", seq(n.samp))
  
  # Specimen abundance
  set.seed(obs.seed)
  obs.list=lapply(1:n.samp, function(i) 
    phyloseq:::rarefaction_subsample(x=abn.mat[, i], sample.size=obs.total[i]))
  obs.mat=Reduce('cbind', obs.list)
  
  # Prepare output data sets
  abn.dat=data.frame(abn.mat, row.names = NULL)
  rownames(abn.dat)=paste0("taxon", seq(n.taxa))
  colnames(abn.dat)=paste0("sub", seq(n.samp))
  
  obs.dat=data.frame(obs.mat, row.names = NULL)
  rownames(obs.dat)=paste0("taxon", seq(n.taxa))
  colnames(obs.dat)=paste0("sub", seq(n.samp))
  
  grp.ind=c(rep(1, n.samp.grp1), rep(2, n.samp.grp2))
  names(grp.ind)=paste0("sub", seq(n.samp))
  
  names(diff.ind)=paste0("taxon", seq(n.taxa))
  
  c.mult=obs.total/abn.total
  names(c.mult)=paste0("sub", seq(n.samp))
  
  test.data=list(temp.dat, abn.dat, obs.dat, effect.size, grp.ind, 
                 diff.ind, out.ind, c.mult, abn.total, obs.total)
  names(test.data)=c("template", "pop.abn", "obs.abn", "effect.size", "grp", 
                     "diff.taxa", "outlier", "mult", "abn.total", "obs.total")
  return(test.data)
}
```

# 2. Data Pre-Processing

```{r, message=FALSE, warning=FALSE, comment=NA}
feature_table_pre_process=function(feature.table, meta.data, sample.var, group.var, pre.cut, neg.lb){
  feature.table=as.data.frame(feature.table)
  meta.data=as.data.frame(meta.data)
  meta.data[]=lapply(meta.data, function(x) if(is.factor(x)) factor(x) else x)
  
  sample.ID=colnames(feature.table)
  meta.data=meta.data[match(sample.ID, meta.data[, sample.var]), ]
  
  # 1. Identify outliers within each taxon
  group=as.factor(meta.data[, group.var])
  group.name=levels(group)
  grp.ind.origin=lapply(1:nlevels(group), function(i) which(group==group.name[i]))
  n.grp.origin=length(grp.ind.origin)
  n.samp.grp.origin=sapply(grp.ind.origin, length)
  feature.table=feature.table[, unlist(grp.ind.origin)]

  z=log(feature.table+1)
  f=z; f[f==0]=NA; f=colMeans(f, na.rm = T)
  f.mean=unlist(tapply(f, rep(1:n.grp.origin, n.samp.grp.origin), mean))
  e=f-rep(f.mean, n.samp.grp.origin)
  y=t(t(z)-e)
  
  outlier_check=function(x){
    mu1=quantile(x, 0.25); mu2=quantile(x, 0.75)
    sigma1=quantile(x, 0.75)-quantile(x, 0.25); sigma2=sigma1
    pi=0.75
    n=length(x)
    epsilon=100
    tol=1e-5
    score=pi*dnorm(x, mean = mu1, sd=sigma1)/((1-pi)*dnorm(x, mean = mu2, sd=sigma2))
    while (epsilon>tol) {
      grp1.ind=score>=1
      mu1.new=mean(x[grp1.ind]); mu2.new=mean(x[!grp1.ind])
      sigma1.new=sd(x[grp1.ind]); if(is.na(sigma1.new)) sigma1.new=0
      sigma2.new=sd(x[!grp1.ind]); if(is.na(sigma2.new)) sigma2.new=0
      pi.new=sum(grp1.ind)/n
      
      para=c(mu1.new, mu2.new, sigma1.new, sigma2.new, pi.new)
      if(any(is.na(para))) break
      
      score=pi.new*dnorm(x, mean = mu1.new, sd=sigma1.new)/
        ((1-pi.new)*dnorm(x, mean = mu2.new, sd=sigma2.new))
      
      epsilon=sqrt((mu1-mu1.new)^2+(mu2-mu2.new)^2+
                     (sigma1-sigma1.new)^2+(sigma2-sigma2.new)^2+(pi-pi.new)^2)
      mu1=mu1.new; mu2=mu2.new; sigma1=sigma1.new; sigma2=sigma2.new; pi=pi.new
    }
    
    if(mu1+1.96*sigma1<mu2-1.96*sigma2){
      if(pi>0.85){
        out.ind=(!grp1.ind)
      }else if(pi<0.15){
        out.ind=grp1.ind
      }else{
        out.ind=rep(FALSE, n)
      }
    }else{
      out.ind=rep(FALSE, n)
    }
    return(out.ind)
  }
  feature.table.out=t(apply(y, 1, function(i)
    unlist(tapply(i, rep(1:n.grp.origin, n.samp.grp.origin), function(j) outlier_check(j)))))
  feature.table[feature.table.out]=NA
  
  # 2. Filter out taxa with zeros >= pre.cut
  taxa.zero.prop=apply(feature.table, 1, function(x) sum(x==0, na.rm = T)/length(x[!is.na(x)]))
  filter.taxa=which(taxa.zero.prop>=pre.cut)
  if(length(filter.taxa)>0){
    feature.table=feature.table[-filter.taxa, ]
  }
  
  # 3. Filter out subjects with library size < 1000
  library.size=colSums(feature.table, na.rm = T)
  sample.ID=colnames(feature.table)
  meta.data=meta.data[match(sample.ID, meta.data[, sample.var]), ]
  
  if(any(library.size<1000)){
    filter.subject=which(library.size<1000)
    feature.table=feature.table[, -filter.subject]
    meta.data=meta.data[-filter.subject, ]
  }
  
  # 4. Re-order the OTU table
  group=as.factor(meta.data[, group.var])
  group.name=levels(group)
  grp.ind=lapply(1:nlevels(group), function(i) which(group==group.name[i]))
  n.grp=length(grp.ind)
  n.samp.grp=sapply(grp.ind, length)
  
  n.taxa=nrow(feature.table)
  taxa.id=rownames(feature.table)
  n.samp=ncol(feature.table)
  
  # 5. Identify taxa containing structure zeros
  present.table=as.matrix(feature.table)
  present.table[is.na(present.table)]=0
  present.table[present.table!=0]=1
  
  p.hat.mat=t(apply(present.table, 1, function(x)
    unlist(tapply(x, rep(1:n.grp, n.samp.grp), function(y) mean(y, na.rm = T)))))
  sample.size=t(apply(feature.table, 1, function(x)
    unlist(tapply(x, rep(1:n.grp, n.samp.grp), function(y) length(y[!is.na(y)])))))
  p.hat.lo.mat=p.hat.mat-1.96*sqrt(p.hat.mat*(1-p.hat.mat)/sample.size)
  colnames(p.hat.mat)=levels(group)
  colnames(p.hat.lo.mat)=levels(group)
  
  struc.zero=matrix(0, nrow = n.taxa, ncol = n.grp)
  struc.zero[p.hat.mat==0]=1
  # Whether we need to classify a taxon into structural zero by its negative lower bound?
  if(neg.lb) struc.zero[p.hat.lo.mat<=0]=1
  rownames(struc.zero)=taxa.id
  colnames(struc.zero)=paste0("Structural Zero in ", levels(group))
  
  # 6. Return results
  res=list(feature.table=feature.table, library.size=library.size, 
           group.name=group.name, group.ind=grp.ind, structure.zeros=struc.zero)
  return(res)
}
```

# 3. ANCOM-BC

```{r, message=FALSE, warning=FALSE, comment=NA}
ANCOM_BC=function(feature.table, grp.name, grp.ind, struc.zero, adj.method, 
                  tol.EM, max.iterNum, alpha){
  n.taxa.origin=nrow(feature.table)
  taxa.id.origin=rownames(feature.table)
  n.samp=ncol(feature.table)
  sample.id=colnames(feature.table)
  
  n.grp=length(grp.ind)
  n.samp.grp=sapply(grp.ind, length)
  
  ### 0. Separate out taxa with no structural zeros
  info.taxa.pos=which(apply(struc.zero, 1, function(x) all(x==0)))
  O=feature.table[info.taxa.pos, ]
  n.taxa=nrow(O)
  taxa.id=rownames(O)
  n.samp=ncol(O)
  y=log(O+1)
  
  ### 1. Parameters estimates
  d=rep(0, n.samp)
  mu=t(apply(y, 1, function(i) tapply(i, rep(1:n.grp, n.samp.grp), function(j)
    mean(j, na.rm = T))))
  iterNum=0
  epsilon=100
  while (epsilon>tol.EM&iterNum<max.iterNum) {
    # Updating mu
    mu.new=t(apply(t(t(y)-d), 1, function(i) tapply(i, rep(1:n.grp, n.samp.grp), function(j)
      mean(j, na.rm = T))))
    
    # Updating d
    d.new=colMeans(y-mu.new[, rep(1:ncol(mu.new), times = n.samp.grp)], na.rm = T)
    
    # Iteration
    epsilon=sqrt(sum((mu.new-mu)^2)+sum((d.new-d)^2))
    iterNum=iterNum+1
    
    mu=mu.new
    d=d.new
  }
  
  mu.var.raw=(y-t(t(mu[, rep(1:ncol(mu), times = n.samp.grp)])+d))^2
  mu.var=t(apply(mu.var.raw, 1, function(x) tapply(x, rep(1:n.grp, n.samp.grp), function(y)
    mean(y, na.rm = T))))
  sample.size=t(apply(y, 1, function(x)
    unlist(tapply(x, rep(1:n.grp, n.samp.grp), function(y) length(y[!is.na(y)])))))
  mu.var=mu.var/sample.size
  
  ### 2. Estimate the between-group by E-M algorithm
  Delta=mu[, 1]-mu[, 2]
  Delta.var.est=rowSums(mu.var)
  
  ## 2.1 Initials
  pi1_0=0.125
  pi2_0=0.75
  pi3_0=0.125
  delta_0=mean(Delta[Delta>=quantile(Delta, 0.25, na.rm = T)&
                       Delta<=quantile(Delta, 0.75, na.rm = T)], na.rm = T)
  d1_0=mean(Delta[Delta<quantile(Delta, 0.125, na.rm = T)], na.rm = T)
  d2_0=mean(Delta[Delta>quantile(Delta, 0.875, na.rm = T)], na.rm = T)
  psi1.sq_0=var(Delta[Delta<quantile(Delta, 0.125, na.rm = T)], na.rm = T)
  if(is.na(psi1.sq_0)|psi1.sq_0==0) psi1.sq_0=1
  psi2.sq_0=var(Delta[Delta>quantile(Delta, 0.875, na.rm = T)], na.rm = T)
  if(is.na(psi2.sq_0)|psi2.sq_0==0) psi2.sq_0=1
  
  ## 2.2 Apply E-M algorithm
  # 2.21 Store all paras in vectors/matrices
  pi1.vec=c(pi1_0); pi2.vec=c(pi2_0); pi3.vec=c(pi3_0)
  delta.vec=c(delta_0); d1.vec=c(d1_0); d2.vec=c(d2_0)
  psi1.sq.vec=c(psi1.sq_0); psi2.sq.vec=c(psi2.sq_0)
  
  # 2.22 E-M iteration
  iterNum=0
  epsilon=100
  sigmai.sq=Delta.var.est
  while (epsilon>tol.EM&iterNum<max.iterNum) {
    # print(iterNum)
    ## Current value of paras
    pi1=pi1.vec[length(pi1.vec)]; pi2=pi2.vec[length(pi2.vec)]; pi3=pi3.vec[length(pi3.vec)]
    delta=delta.vec[length(delta.vec)]; 
    d1=d1.vec[length(d1.vec)]; d2=d2.vec[length(d2.vec)]
    psi1.sq=psi1.sq.vec[length(psi1.sq.vec)]; psi2.sq=psi2.sq.vec[length(psi2.sq.vec)]
    
    ## E-step
    pdf1=sapply(seq(n.taxa), function(i) dnorm(Delta[i], delta+d1, sqrt(sigmai.sq[i]+psi1.sq)))
    pdf2=sapply(seq(n.taxa), function(i) dnorm(Delta[i], delta, sqrt(sigmai.sq[i])))
    pdf3=sapply(seq(n.taxa), function(i) dnorm(Delta[i], delta+d2, sqrt(sigmai.sq[i]+psi2.sq)))
    r1i=pi1*pdf1/(pi1*pdf1+pi2*pdf2+pi3*pdf3); r1i[is.na(r1i)]=0
    r2i=pi2*pdf2/(pi1*pdf1+pi2*pdf2+pi3*pdf3); r2i[is.na(r2i)]=0
    r3i=pi3*pdf3/(pi1*pdf1+pi2*pdf2+pi3*pdf3); r3i[is.na(r3i)]=0
    
    ## M-step
    pi1_new=mean(r1i, na.rm = T); pi2_new=mean(r2i, na.rm = T); pi3_new=mean(r3i, na.rm = T)
    delta_new=sum(r1i*(Delta-d1)/(sigmai.sq+psi1.sq)+r2i*Delta/sigmai.sq+
                    r3i*(Delta-d2)/(sigmai.sq+psi2.sq), na.rm = T)/
      sum(r1i/(sigmai.sq+psi1.sq)+r2i/sigmai.sq+r3i/(sigmai.sq+psi2.sq), na.rm = T)
    d1_new=min(sum(r1i*(Delta-delta)/(sigmai.sq+psi1.sq), na.rm = T)/
                 sum(r1i/(sigmai.sq+psi1.sq), na.rm = T), 0)
    if(is.nan(d1_new)) d1_new=0
    d2_new=max(sum(r3i*(Delta-delta)/(sigmai.sq+psi2.sq), na.rm = T)/
                 sum(r3i/(sigmai.sq+psi2.sq), na.rm = T), 0)
    if(is.nan(d2_new)) d2_new=0
    
    # Nelder-Mead simplex algorithm for psi1.sq, psi2.sq, and sigmai.sq
    obj.psi1.sq=function(x){
      log.pdf=log(sapply(seq(n.taxa), function(i) dnorm(Delta[i], delta+d1, sqrt(sigmai.sq[i]+x))))
      log.pdf[is.infinite(log.pdf)]=0
      -sum(r1i*log.pdf, na.rm = T)
    }
    psi1.sq_new=neldermead(x0 = psi1.sq, fn = obj.psi1.sq, lower = 0)$par
    
    obj.psi2.sq=function(x){
      log.pdf=log(sapply(seq(n.taxa), function(i) dnorm(Delta[i], delta+d2, sqrt(sigmai.sq[i]+x))))
      log.pdf[is.infinite(log.pdf)]=0
      -sum(r3i*log.pdf, na.rm = T)
    }
    psi2.sq_new=neldermead(x0 = psi2.sq, fn = obj.psi2.sq, lower = 0)$par
    
    ## Merge to the paras vectors/matrices
    pi1.vec=c(pi1.vec, pi1_new); pi2.vec=c(pi2.vec, pi2_new); pi3.vec=c(pi3.vec, pi3_new)
    delta.vec=c(delta.vec, delta_new)
    d1.vec=c(d1.vec, d1_new); d2.vec=c(d2.vec, d2_new)
    psi1.sq.vec=c(psi1.sq.vec, psi1.sq_new); psi2.sq.vec=c(psi2.sq.vec, psi2.sq_new)
    
    ## Calculate the new epsilon
    epsilon=sqrt((pi1_new-pi1)^2+(pi2_new-pi2)^2+(pi3_new-pi3)^2+(delta_new-delta)^2+
                   (d1_new-d1)^2+(d2_new-d2)^2+(psi1.sq_new-psi1.sq)^2+(psi2.sq_new-psi2.sq)^2)
    iterNum=iterNum+1
  }
  bias.est=delta.vec[length(delta.vec)]
  
  ### 4. Test results
  ## 4.1 Results for taxa with non-structural zeros
  W.numerator=mu[, 1]-mu[, 2]-bias.est
  W.denominator=mu.var[, 1]+mu.var[, 2]
  
  W=W.numerator/sqrt(W.denominator)
  p.val=sapply(W, function(x) 2*pnorm(abs(x), mean=0, sd=1, lower.tail = F))
  q.val=p.adjust(p.val, method = adj.method)
  q.val[is.na(q.val)]=1
  
  res.nonstrc.zero=data.frame(W.numerator, se=sqrt(W.denominator), W, p.val, q.val)
  
  ## 4.2 Results for taxa with structural zeros
  if(length(info.taxa.pos)<n.taxa.origin){
    O.strc.zero=feature.table[-info.taxa.pos, ]
    ind.strc.zero=struc.zero[-info.taxa.pos, rep(1:n.grp, times = n.samp.grp)]
    O.strc.zero.adj=O.strc.zero*(1-ind.strc.zero)
    y.strc.zero=log(O.strc.zero.adj+1)
    d.strc.zero=t(t(1-ind.strc.zero)*d)
    y.strc.zero.adj=y.strc.zero-d.strc.zero
    mu.strc.zero=t(apply(y.strc.zero.adj, 1, function(i) 
      tapply(i, rep(1:n.grp, n.samp.grp), function(j) mean(j, na.rm = T))))
    # Make it the relative mean difference (with related to the smallest value)
    mu.strc.zero.adj=mu.strc.zero
    mu.strc.zero.adj[mu.strc.zero.adj==0]=NA
    mu.strc.zero.adj=t(t(mu.strc.zero.adj)+abs(apply(mu.strc.zero, 2, min)))
    mu.strc.zero.adj[is.na(mu.strc.zero.adj)]=0
    
    res.strc.zero=data.frame(W.numerator=mu.strc.zero.adj[, 1]-mu.strc.zero.adj[, 2], 
                             se=0, W=Inf, p.val=0, q.val=0)
  }else{
    res.strc.zero=NA
  }
  
  ## 4.3 Combine these results together
  res=data.frame(W.numerator=Inf, se=0, W=Inf, 
                 p.val=rep(0, n.taxa.origin), q.val=rep(0, n.taxa.origin))
  res[info.taxa.pos, ]=res.nonstrc.zero
  res[-info.taxa.pos, ]=res.strc.zero
  res=mutate(res, diff.abn=ifelse(q.val<alpha, TRUE, FALSE))
  
  colnames(res)=c(paste0("mean.difference (", grp.name[1], " - ", grp.name[2], ")"), 
                  "se", paste0("effect.size (", grp.name[1], " - ", grp.name[2], ")"), 
                  "p.val", "q.val", "diff.abn")
  out=list(feature.table=feature.table, res=res, d=d, mu=mu, bias.est=bias.est)
  return(out)
}
```

# 4. Monte Carlo Simulations

## 4.1 Simulation Settings

```{r, message=FALSE, warning=FALSE, comment=NA}
# The number of taxa, sampling depth, and sample size
n.taxa=1000; library.size="small"; n.samp=c("20_30", "50_50")

# The proportion of differentially abundant taxa
prop.diff=c(0.05, 0.15, 0.25)

# Set seeds
iterNum=100
abn.seed=seq(iterNum)

# Define the simulation parameters combinations
simparams=expand.grid(n.taxa, n.samp, prop.diff, abn.seed, library.size)
colnames(simparams)=c("n.taxa", "n.samp", "prop.diff", "abn.seed", "library.size")
simparams=simparams%>%mutate(obs.seed=abn.seed+1)
simparams=simparams%>%separate(col = n.samp, into = c("n.samp.grp1", "n.samp.grp2"), sep = "_")
simparams=simparams%>%arrange(n.taxa, n.samp.grp1, prop.diff, abn.seed, obs.seed)
simparams.list=apply(simparams, 1, paste0, collapse="_")

simparamslabels=c("n.taxa", "n.samp.grp1", "n.samp.grp2","prop.diff", 
                  "abn.seed", "library.size", "obs.seed")
```

## 4.2 ANCOM-BC

```{r, message=FALSE, warning=FALSE, comment=NA, eval=FALSE}
library(doParallel)
library(foreach)

detectCores()
myCluster=makeCluster(2, type = "FORK")
registerDoParallel(myCluster)

start_time <- Sys.time()
simlist=foreach(i = simparams.list, .combine = 'cbind') %dopar% {
  # i = simparams.list[1]
  print(i)
  params = strsplit(i, "_")[[1]]
  names(params) <- simparamslabels
  
  # Paras for data generation
  n.taxa=as.numeric(params["n.taxa"])
  n.samp.grp1=as.numeric(params["n.samp.grp1"])
  n.samp.grp2=as.numeric(params["n.samp.grp2"])
  prop.diff=as.numeric(params["prop.diff"])
  abn.seed=as.numeric(params["abn.seed"])
  obs.seed=as.numeric(params["obs.seed"])
  library.size=params["library.size"]
  
  # Data generation
  test.dat=abn.tab.gen1(n.taxa, n.samp.grp1, n.samp.grp2, prop.diff, abn.seed, obs.seed,
                        struc.zero.prop=0.2, out.zero.prop=0.05, library.size)
  obs.abn=test.dat$obs.abn
  meta.data=cbind(Sample.ID=paste0("sub", seq(n.samp.grp1+n.samp.grp2)), 
                  group=rep(c(1, 2), c(n.samp.grp1, n.samp.grp2)))
  
  # Pre-processing
  feature.table=obs.abn; sample.var="Sample.ID"; group.var="group"; pre.cut=0.90; neg.lb=FALSE
  pre.process=feature_table_pre_process(feature.table, meta.data, sample.var, 
                                        group.var, pre.cut, neg.lb)
  feature.table=pre.process$feature.table
  group.name=pre.process$group.name
  group.ind=pre.process$group.ind
  struc.zero=pre.process$structure.zeros
  
  # Paras for ANCOM-BC
  grp.name=group.name; grp.ind=group.ind; adj.method="bonferroni"
  tol.EM=1e-5; max.iterNum=100; alpha=0.05
  
  # Run ANCOM-BC
  suppressWarnings(out <- try(ANCOM_BC(feature.table, grp.name, grp.ind, struc.zero,
                                       adj.method, tol.EM, max.iterNum, alpha), 
                              silent = TRUE))
  if (inherits(out, "try-error")) {
    FDR=NA; power=NA
  }else{
    res=cbind(out$res, diff.ind=test.dat$diff.taxa[rownames(out$feature.table)])
    
    # FDR
    FDR=ifelse(sum(res$diff.abn, na.rm = T)==0, 0, 
               sum(ifelse(res$diff.ind==0&res$diff.abn, 1, 0), na.rm = T)/
                 sum(res$diff.abn, na.rm = T))
    
    # Power
    power=sum(ifelse(res$diff.ind!=0&res$diff.abn, 1, 0), na.rm = T)/
      sum(res$diff.ind!=0, na.rm = T)
  }
  c(FDR, power)
}
end_time <- Sys.time()
end_time - start_time

stopCluster(myCluster)
write_csv(simlist, "fdr_power_ancom_bc_large.csv")
```

## 4.3 ANCOM

```{r, message=FALSE, warning=FALSE, comment=NA, eval=FALSE}
library(doParallel)
library(foreach)

detectCores()
myCluster=makeCluster(10, type = "FORK")
registerDoParallel(myCluster)

start_time <- Sys.time()
simlist=foreach(i = simparams.list, .combine = 'cbind') %dopar% {
  # i = simparams.list[1]
  print(i)
  params = strsplit(i, "_")[[1]]
  names(params) <- simparamslabels
  
  # Paras for data generation
  n.taxa=as.numeric(params["n.taxa"])
  n.samp.grp1=as.numeric(params["n.samp.grp1"])
  n.samp.grp2=as.numeric(params["n.samp.grp2"])
  prop.diff=as.numeric(params["prop.diff"])
  abn.seed=as.numeric(params["abn.seed"])
  obs.seed=as.numeric(params["obs.seed"])
  library.size=params["library.size"]
  
  # Data generation
  test.dat=abn.tab.gen1(n.taxa, n.samp.grp1, n.samp.grp2, prop.diff, abn.seed, obs.seed, 
                        struc.zero.prop=0.2, out.zero.prop=0.05, library.size)
  meta.data=cbind(Sample.ID=paste0("sub", seq(n.samp.grp1+n.samp.grp2)), 
                  group=rep(c(1, 2), c(n.samp.grp1, n.samp.grp2)))
  feature.table.origin=test.dat$obs.abn
  
  # Pre-processing
  pre.process=feature_table_pre_process(feature.table.origin, meta.data, 
                                        sample.var="Sample.ID", group.var="group",
                                        pre.cut=0.90, neg.lb = FALSE)
  struc.zero=pre.process$structure.zeros
  num.struc.zero=apply(struc.zero, 1, sum)
  feature.table=pre.process$feature.table
  s0=rownames(feature.table)[which(num.struc.zero==0)]
  s1=rownames(feature.table)[which(num.struc.zero==1)]
  
  # Run ANCOM
  # Format for ANCOM: rows = subjects, cols=taxa
  feature.table.sub=t(feature.table[s0, ])

  res.W=ANCOM.main(OTUdat=feature.table.sub, Vardat=meta.data, 
                   main.var="group", sig=0.05, prev.cut=1.01)

  res=data.frame(otu.names=c(s0, s1), W_stat=Inf, detected_0.9=TRUE,
                 detected_0.8=TRUE, detected_0.7=TRUE, detected_0.6=TRUE)
  res[match(as.character(res.W$otu.names), res$otu.names), ]=res.W
  res$diff.ind=test.dat$diff.taxa[c(s0, s1)]
  
  # FDR
  FDR=ifelse(sum(res$detected_0.7, na.rm = T)==0, 0, 
             sum(ifelse(res$diff.ind==0&res$detected_0.7, 1, 0), na.rm = T)/
               sum(res$detected_0.7, na.rm = T))
  
  # Power
  power=sum(ifelse(res$diff.ind!=0&res$detected_0.7, 1, 0), na.rm = T)/
    sum(res$diff.ind!=0, na.rm = T)
  
  c(FDR, power)
}
end_time <- Sys.time()
end_time - start_time

stopCluster(myCluster)
write_csv(simlist, "fdr_power_ancom_large.csv")
```

## 4.4 DESeq2

```{r, message=FALSE, warning=FALSE, comment=NA, eval=FALSE}
library(DESeq2)
library(doParallel)
library(foreach)

start_time <- Sys.time()
simlist=foreach(i = simparams.list, .combine = 'cbind') %do% {
  # i=simparams.list[[1]]
  print(i)
  params = strsplit(i, "_")[[1]]
  names(params) <- simparamslabels
  
  # Paras for data generation
  n.taxa=as.numeric(params["n.taxa"])
  n.samp.grp1=as.numeric(params["n.samp.grp1"])
  n.samp.grp2=as.numeric(params["n.samp.grp2"])
  prop.diff=as.numeric(params["prop.diff"])
  abn.seed=as.numeric(params["abn.seed"])
  obs.seed=as.numeric(params["obs.seed"])
  library.size=params["library.size"]
  
  # Data generation
  test.dat=abn.tab.gen1(n.taxa, n.samp.grp1, n.samp.grp2, prop.diff, abn.seed, obs.seed, 
                        struc.zero.prop=0.2, out.zero.prop=0.05, library.size)
  
  # Prepare data for DESeq2
  countdata=test.dat$obs.abn # Format for DESeq2: taxa are rows
  coldata=data.frame(group=as.factor(rep(c(1, 2), c(n.samp.grp1, n.samp.grp2))))
  rownames(coldata)=paste0("sub", seq(n.samp.grp1+n.samp.grp2))
  
  zero.threshold=0.90
  taxa.info.ind=apply(countdata, 1, function(x) sum(x==0)/(n.samp.grp1+n.samp.grp2))
  feature_table=round(countdata[which(taxa.info.ind<zero.threshold), ])+1L
  
  count.table=DESeqDataSetFromMatrix(
    countData = feature_table, colData = coldata, design = ~ group)
  
  # Run DESeq2
  suppressWarnings(dds <- try(DESeq(count.table, quiet = TRUE), silent = TRUE))
  if (inherits(dds, "try-error")) {
    # If the parametric fit failed, try the local.
    suppressWarnings(dds <- try(DESeq(count.table, fitType = "local", quiet = TRUE), silent = TRUE))
    if (inherits(dds, "try-error")) {
      # If local fails, try the mean
      suppressWarnings(dds <- try(DESeq(count.table, fitType = "mean", quiet = TRUE), silent = TRUE))
    }
  }
  if (inherits(dds, "try-error")) {
    # If still bad
    FDR=NA; power=NA
  }else{
    res = results(dds)
    res$id = rownames(res)
    # Some DESeq2 results (for example) had NA adjusted p-values, so replace them with 1
    res[is.na(res[, "padj"]), "padj"] = 1
    
    res=data.frame(taxa=res$id,
                   diff.test=ifelse(res$padj<0.05, 1, 0), 
                   diff.ind=test.dat$diff.taxa[which(taxa.info.ind<zero.threshold)],
                   effec.size=test.dat$effect.size[which(taxa.info.ind<zero.threshold)])
    FDR=ifelse(sum(res$diff.test==1, na.rm = T)==0, 0,
               sum(ifelse(res$diff.ind==0&res$diff.test==1, 1, 0), na.rm = T)/
                 sum(res$diff.test==1, na.rm = T))
    power=sum(ifelse(res$diff.ind!=0&res$diff.test==1, 1, 0), na.rm = T)/
      sum(res$diff.ind!=0, na.rm = T)
    
  }
  
  c(FDR, power)
}
end_time <- Sys.time()
end_time - start_time

write_csv(simlist, "fdr_power_deseq2_large.csv")
```

## 4.5 edgeR

```{r, message=FALSE, warning=FALSE, comment=NA, eval=FALSE}
library(edgeR)
library(doParallel)
library(foreach)

start_time <- Sys.time()
simlist=foreach(i = simparams.list, .combine = 'cbind') %do% {
  # i = simparams.list[[1]]
  print(i)
  params = strsplit(i, "_")[[1]]
  names(params) <- simparamslabels
  
  # Paras for data generation
  n.taxa=as.numeric(params["n.taxa"])
  n.samp.grp1=as.numeric(params["n.samp.grp1"])
  n.samp.grp2=as.numeric(params["n.samp.grp2"])
  prop.diff=as.numeric(params["prop.diff"])
  abn.seed=as.numeric(params["abn.seed"])
  obs.seed=as.numeric(params["obs.seed"])
  library.size=params["library.size"]
  
  # Data generation
  test.dat=abn.tab.gen1(n.taxa, n.samp.grp1, n.samp.grp2, prop.diff, abn.seed, obs.seed, 
                        struc.zero.prop=0.2, out.zero.prop=0.05, library.size)
  
  # Prepare data for edgeR
  groupdata=factor(rep(c(1, 2), c(n.samp.grp1, n.samp.grp2)))
  countdata=test.dat$obs.abn; meta.data=data.frame(group=groupdata)
  
  zero.threshold=0.90
  taxa.info.ind=apply(countdata, 1, function(x) sum(x==0)/(n.samp.grp1+n.samp.grp2))
  countdata=countdata[which(taxa.info.ind<zero.threshold), ]+1L
  
  d=DGEList(counts = countdata, group = groupdata)
  d=calcNormFactors(d)
  design.mat=model.matrix(~ 0 + d$samples$group)
  colnames(design.mat)=levels(d$samples$group)
  
  d=estimateDisp(d, design.mat)
  fit=glmQLFit(d, design.mat)
  qlf=glmQLFTest(fit, contrast=c(1, -1))
  out=data.frame(taxa=rownames(topTags(qlf, n=nrow(countdata))$table), 
                 FDR=topTags(qlf, n=nrow(countdata))$table$FDR)
  out=out[match(rownames(countdata), as.character(out$taxa)), ]
  out$FDR[is.na(out$FDR)]=1
  
  res=data.frame(diff.test=ifelse(out$FDR<0.05, 1, 0), 
                 diff.ind=test.dat$diff.taxa[which(taxa.info.ind<zero.threshold)])
  
  # FDR
  FDR=ifelse(sum(res$diff.test==1, na.rm = T)==0, 0, 
             sum(ifelse(res$diff.ind==0&res$diff.test==1, 1, 0), na.rm = T)/
               sum(res$diff.test==1, na.rm = T))
  
  # Power
  power=sum(ifelse(res$diff.ind!=0&res$diff.test==1, 1, 0), na.rm = T)/
    sum(res$diff.ind!=0, na.rm = T)
  
  c(FDR, power)
}
end_time <- Sys.time()
end_time - start_time

write_csv(simlist, "fdr_power_edger_large.csv")
```

## 4.6 metagenomeSeq: Zero-inflated log-normal mixture model

```{r, message=FALSE, warning=FALSE, comment=NA, eval=FALSE}
library(metagenomeSeq)
library(doParallel)
library(foreach)

start_time <- Sys.time()
simlist=foreach(i = simparams.list, .combine = 'cbind') %do% {
  # i=simparams.list[[665]]
  print(i)
  params = strsplit(i, "_")[[1]]
  names(params) <- simparamslabels
  
  # Paras for data generation
  n.taxa=as.numeric(params["n.taxa"])
  n.samp.grp1=as.numeric(params["n.samp.grp1"])
  n.samp.grp2=as.numeric(params["n.samp.grp2"])
  prop.diff=as.numeric(params["prop.diff"])
  abn.seed=as.numeric(params["abn.seed"])
  obs.seed=as.numeric(params["obs.seed"])
  library.size=params["library.size"]
  
  # Data generation
  test.dat=abn.tab.gen1(n.taxa, n.samp.grp1, n.samp.grp2, prop.diff, abn.seed, obs.seed, 
                        struc.zero.prop=0.2, out.zero.prop=0.05, library.size)
  
  # Prepare data for metagenomeSeq
  meta.data=data.frame(group=rep(c(1, 2), c(n.samp.grp1, n.samp.grp2)))
  rownames(meta.data)=paste0("sub", seq(n.samp.grp1+n.samp.grp2))
  
  countdata=test.dat$obs.abn
  
  zero.threshold=0.90
  taxa.info.ind=apply(countdata, 1, function(x) sum(x==0)/(n.samp.grp1+n.samp.grp2))
  countdata=countdata[which(taxa.info.ind<zero.threshold), ]+1L
  
  # Run metagenomeSeq
  phenotypeData = AnnotatedDataFrame(meta.data)
  obj = newMRexperiment(countdata, phenoData=phenotypeData, featureData=NULL)
  
  # Calculating normalization factors
  obj = cumNorm(obj)
  
  # Zero-inflated Log-Normal mixture model
  pd = pData(obj)
  mod = model.matrix(~group, data = pd)
  
  suppressWarnings(fit <- try(fitFeatureModel(obj, mod)))
  if (inherits(fit, "try-error")) {
    power=NA; FDR=NA
  } else{
    out=MRcoefs(fit, number = nrow(countdata))
    out=data.frame(taxa=rownames(out), FDR=out$adjPvalues)
    out=out[match(rownames(countdata), as.character(out$taxa)), ]
    out$FDR[is.na(out$FDR)]=1
    
    res=data.frame(taxa=out$taxa,
                   diff.test=ifelse(out$FDR<0.05, 1, 0), 
                   diff.ind=test.dat$diff.taxa[which(taxa.info.ind<zero.threshold)])
    FDR=ifelse(sum(res$diff.test==1, na.rm = T)==0, 0,
               sum(ifelse(res$diff.ind==0&res$diff.test==1, 1, 0), na.rm = T)/
                 sum(res$diff.test==1, na.rm = T))
    power=sum(ifelse(res$diff.ind!=0&res$diff.test==1, 1, 0), na.rm = T)/
      sum(res$diff.ind!=0, na.rm = T)
  }
  
  c(FDR, power)
}
end_time <- Sys.time()
end_time - start_time

write_csv(simlist, "fdr_power_zilg_large.csv")
```

## 4.7 metagenomeSeq: Zero-inflated gaussian mixture model

```{r, message=FALSE, warning=FALSE, comment=NA, eval=FALSE}
library(metagenomeSeq)
library(doParallel)
library(foreach)

start_time <- Sys.time()
simlist=foreach(i = simparams.list, .combine = 'cbind') %do% {
  # i=simparams.list[[1]]
  print(i)
  params = strsplit(i, "_")[[1]]
  names(params) <- simparamslabels
  
  # Paras for data generation
  n.taxa=as.numeric(params["n.taxa"])
  n.samp.grp1=as.numeric(params["n.samp.grp1"])
  n.samp.grp2=as.numeric(params["n.samp.grp2"])
  prop.diff=as.numeric(params["prop.diff"])
  abn.seed=as.numeric(params["abn.seed"])
  obs.seed=as.numeric(params["obs.seed"])
  library.size=params["library.size"]
  
  # Data generation
  test.dat=abn.tab.gen1(n.taxa, n.samp.grp1, n.samp.grp2, prop.diff, abn.seed, obs.seed, 
                        struc.zero.prop=0.2, out.zero.prop=0.05, library.size)
  
  # Prepare data for metagenomeSeq
  meta.data=data.frame(group=rep(c(1, 2), c(n.samp.grp1, n.samp.grp2)))
  rownames(meta.data)=paste0("sub", seq(n.samp.grp1+n.samp.grp2))
  
  countdata=test.dat$obs.abn
  
  zero.threshold=0.90
  taxa.info.ind=apply(countdata, 1, function(x) sum(x==0)/(n.samp.grp1+n.samp.grp2))
  countdata=countdata[which(taxa.info.ind<zero.threshold), ]+1L
  
  # Run metagenomeSeq
  phenotypeData = AnnotatedDataFrame(meta.data)
  obj = newMRexperiment(countdata, phenoData=phenotypeData, featureData=NULL)
  
  # Calculating normalization factors
  obj = cumNorm(obj)
  
  # Zero-inflated Log-Normal mixture model
  pd = pData(obj)
  mod = model.matrix(~group, data = pd)
  
  settings = zigControl(maxit = 100, verbose = F)
  suppressWarnings(fit <- try(fitZig(obj, mod, useCSSoffset = T, control = settings)))
  if (inherits(fit, "try-error")) {
    power=NA; FDR=NA
  } else{
    out=MRcoefs(fit, number = nrow(countdata))
    out=data.frame(taxa=rownames(out), FDR=out$adjPvalues)
    out=out[match(rownames(countdata), as.character(out$taxa)), ]
    out$FDR[is.na(out$FDR)]=1
    
    res=data.frame(taxa=out$taxa,
                   diff.test=ifelse(out$FDR<0.05, 1, 0), 
                   diff.ind=test.dat$diff.taxa[which(taxa.info.ind<zero.threshold)])
    FDR=ifelse(sum(res$diff.test==1, na.rm = T)==0, 0,
               sum(ifelse(res$diff.ind==0&res$diff.test==1, 1, 0), na.rm = T)/
                 sum(res$diff.test==1, na.rm = T))
    power=sum(ifelse(res$diff.ind!=0&res$diff.test==1, 1, 0), na.rm = T)/
      sum(res$diff.ind!=0, na.rm = T)
  }
  
  c(FDR, power)
}
end_time <- Sys.time()
end_time - start_time

write_csv(simlist, "fdr_power_zig_large.csv")
```

## 4.8 Wilcoxon rank-sum test: Unnormalized

```{r, message=FALSE, warning=FALSE, comment=NA, eval=FALSE}
library(doParallel)
library(foreach)

start_time <- Sys.time()
simlist=foreach(i = simparams.list, .combine = 'cbind') %do% {
  # i=simparams.list[[1]]
  print(i)
  params = strsplit(i, "_")[[1]]
  names(params) <- simparamslabels
  
  # Paras for data generation
  n.taxa=as.numeric(params["n.taxa"])
  n.samp.grp1=as.numeric(params["n.samp.grp1"])
  n.samp.grp2=as.numeric(params["n.samp.grp2"])
  prop.diff=as.numeric(params["prop.diff"])
  abn.seed=as.numeric(params["abn.seed"])
  obs.seed=as.numeric(params["obs.seed"])
  library.size=params["library.size"]
  
  # Generate data
  test.dat=abn.tab.gen1(n.taxa, n.samp.grp1, n.samp.grp2, prop.diff, abn.seed, obs.seed, 
                        struc.zero.prop=0.2, out.zero.prop=0.05, library.size)
  
  meta.data=data.frame(group=rep(c(1, 2), c(n.samp.grp1, n.samp.grp2)))
  countdata=test.dat$obs.abn
  
  zero.threshold=0.90
  taxa.info.ind=apply(countdata, 1, function(x) sum(x==0)/(n.samp.grp1+n.samp.grp2))
  feature_table=round(countdata[which(taxa.info.ind<zero.threshold), ])+1L
  
  # Run wilcox1
  p.val=apply(feature_table, 1, function(x) 
    wilcox.test(x[1:n.samp.grp1], x[(n.samp.grp1+1):(n.samp.grp1+n.samp.grp2)])$p.value)
  FDR=p.adjust(p.val, method = "BH")
  
  res=data.frame(diff.test=ifelse(FDR<0.05, 1, 0), 
                 diff.ind=test.dat$diff.taxa[which(taxa.info.ind<zero.threshold)])
  
  # FDR
  FDR=ifelse(sum(res$diff.test==1, na.rm = T)==0, 0, 
             sum(ifelse(res$diff.ind==0&res$diff.test==1, 1, 0), na.rm = T)/sum(res$diff.test==1, na.rm = T))
  
  # Power
  power=sum(ifelse(res$diff.ind!=0&res$diff.test==1, 1, 0), na.rm = T)/sum(res$diff.ind!=0, na.rm = T)
  
  c(FDR, power)
}
end_time <- Sys.time()
end_time - start_time

write_csv(simlist, "fdr_power_wilcox_un_large.csv")
```

## 4.9 Wilcoxon rank-sum test: TSS

```{r, message=FALSE, warning=FALSE, comment=NA, eval=FALSE}
library(doParallel)
library(foreach)

start_time <- Sys.time()
simlist=foreach(i = simparams.list, .combine = 'cbind') %do% {
  # i=simparams.list[[1]]
  print(i)
  params = strsplit(i, "_")[[1]]
  names(params) <- simparamslabels
  
  # Paras for data generation
  n.taxa=as.numeric(params["n.taxa"])
  n.samp.grp1=as.numeric(params["n.samp.grp1"])
  n.samp.grp2=as.numeric(params["n.samp.grp2"])
  prop.diff=as.numeric(params["prop.diff"])
  abn.seed=as.numeric(params["abn.seed"])
  obs.seed=as.numeric(params["obs.seed"])
  library.size=params["library.size"]
  
  # Generate data
  test.dat=abn.tab.gen1(n.taxa, n.samp.grp1, n.samp.grp2, prop.diff, abn.seed, obs.seed, 
                       struc.zero.prop=0.2, out.zero.prop=0.05, library.size)
  
  meta.data=data.frame(group=rep(c(1, 2), c(n.samp.grp1, n.samp.grp2)))
  countdata=test.dat$obs.abn
  
  zero.threshold=0.90
  taxa.info.ind=apply(countdata, 1, function(x) sum(x==0)/(n.samp.grp1+n.samp.grp2))
  feature_table=round(countdata[which(taxa.info.ind<zero.threshold), ])+1L
  feature_table.scale=apply(feature_table, 2, function(x) x/sum(x))
  
  # Run wilcox2
  p.val=apply(feature_table.scale, 1, function(x) 
    wilcox.test(x[1:n.samp.grp1], x[(n.samp.grp1+1):(n.samp.grp1+n.samp.grp2)])$p.value)
  FDR=p.adjust(p.val, method = "BH")
  
  res=data.frame(diff.test=ifelse(FDR<0.05, 1, 0), 
                 diff.ind=test.dat$diff.taxa[which(taxa.info.ind<zero.threshold)])
  
  # FDR
  FDR=ifelse(sum(res$diff.test==1, na.rm = T)==0, 0, 
             sum(ifelse(res$diff.ind==0&res$diff.test==1, 1, 0), na.rm = T)/sum(res$diff.test==1, na.rm = T))
  
  # Power
  power=sum(ifelse(res$diff.ind!=0&res$diff.test==1, 1, 0), na.rm = T)/sum(res$diff.ind!=0, na.rm = T)
  
  c(FDR, power)
}
end_time <- Sys.time()
end_time - start_time

write_csv(simlist, "fdr_power_wilcox_tss_large.csv")
```

# 5. FDR and Power with the Presence of Large Variable Sampling Fractions

```{r, message=FALSE, warning=FALSE, comment=NA}
library(RColorBrewer)
## Read in original data
dat.ancom_bc=read_csv("../data/sim_fdr_power/fdr_power_ancom_bc_large.csv")
dat.ancom=read_csv("../data/sim_fdr_power/fdr_power_ancom_large.csv")
dat.deseq2=read_csv("../data/sim_fdr_power/fdr_power_deseq2_large.csv")
dat.edger=read_csv("../data/sim_fdr_power/fdr_power_edger_large.csv")
dat.zilg=read_csv("../data/sim_fdr_power/fdr_power_zilg_large.csv")
dat.zig=read_csv("../data/sim_fdr_power/fdr_power_zig_large.csv")
dat.wilcox_un=read_csv("../data/sim_fdr_power/fdr_power_wilcox_un_large.csv")
dat.wilcox_tss=read_csv("../data/sim_fdr_power/fdr_power_wilcox_tss_large.csv")

## Reshaping data
simpattern=distinct(simparams, n.taxa, n.samp.grp1, n.samp.grp2, prop.diff)

data_summary = function(eval_data, method){
  FDR=tapply(as.numeric(eval_data[1, ]), 
             rep(seq(nrow(simpattern)), each=iterNum), function(x) mean(x, na.rm = T))
  FDRSD=tapply(as.numeric(eval_data[1, ]), 
               rep(seq(nrow(simpattern)), each=iterNum), function(x) sd(x, na.rm = T))
  power=tapply(as.numeric(eval_data[2, ]), 
               rep(seq(nrow(simpattern)), each=iterNum), function(x) mean(x, na.rm = T))
  powerSD=tapply(as.numeric(eval_data[2, ]), 
                 rep(seq(nrow(simpattern)), each=iterNum), function(x) sd(x, na.rm = T))
  data_sum = data.frame(FDR, FDRSD, power, powerSD, simpattern, method)
  data_sum = data_sum%>%unite(n.samp.grp, n.samp.grp1, n.samp.grp2, sep = ", ")
  
  return(data_sum)
}

eval.dat.list = list(dat.ancom_bc, dat.ancom, dat.deseq2, dat.edger, 
                     dat.zilg, dat.zig, dat.wilcox_un, dat.wilcox_tss)
method.list = list("ANCOM-BC", "ANCOM", "DESeq2", "edgeR", "ZILG", "ZIG", "Wilcoxon", "Wilcoxon + TSS")

dat.fig.list = vector(mode = "list", length = length(eval.dat.list))
for (i in 1:length(eval.dat.list)) {
  dat.fig.list[[i]] = data_summary(eval.dat.list[[i]], method.list[[i]])
}

## Merge data
dat.fig=Reduce('rbind', dat.fig.list)
dat.fig$n.samp.grp=factor(dat.fig$n.samp.grp)
levels(dat.fig$n.samp.grp)=c("n = 20/30", "n = 50/50")
dat.fig$method=factor(dat.fig$method)
dat.fig$prop.diff=factor(dat.fig$prop.diff)
```

## 5.1 Fig. 3b

```{r, message=FALSE, warning=FALSE, comment=NA}
p=ggplot(dat.fig, aes(x=prop.diff, y=FDR, fill=method)) +
  geom_hline(yintercept=0.05, linetype="dashed", color="black", size = 0.2)+
  scale_y_continuous(breaks = c(0.05, seq(0.2, 1, 0.2)), limits = c(0, 0.8))+
  coord_flip()+facet_grid(.~n.samp.grp)+
  geom_bar(stat="identity", position=position_dodge())+
  labs(x="Proportion of Differentially Abundant Taxa", y="", fill=NULL, title="FDR")+
  scale_fill_brewer(palette="Dark2")+
  theme_bw()+
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        plot.title=element_text(hjust = 0.5),
        strip.background = element_rect(fill="white"),
        legend.position = "bottom")+
  guides(fill=guide_legend(nrow=2, byrow=TRUE))
ggarrange(p, labels = "b")
ggsave("../figures/Figure 3b.pdf", width=6.25, height=5, units='in')
ggsave("../figures/Figure 3b.jpeg", width=6.25, height=5, units='in', dpi = 300)
```

## 5.2 Fig. 3c

```{r, message=FALSE, warning=FALSE, comment=NA}
p=ggplot(dat.fig, aes(x=prop.diff, y=power, fill=method)) +
  scale_y_continuous(breaks = seq(0.2, 1, 0.2), limits = c(0, 1))+
  coord_flip()+facet_grid(.~n.samp.grp)+
  geom_bar(stat="identity", position=position_dodge())+
  labs(x="Proportion of Differentially Abundant Taxa", y="", fill=NULL, title="Power")+
  scale_fill_brewer(palette="Dark2")+
  theme_bw()+
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        strip.background = element_rect(fill="white"),
        plot.title=element_text(hjust = 0.5),
        legend.position = "bottom")+
  guides(fill=guide_legend(nrow=2, byrow=TRUE))
ggarrange(p, labels = "c")
ggsave("../figures/Figure 3c.pdf", width=6.25, height=5, units='in')
ggsave("../figures/Figure 3c.jpeg", width=6.25, height=5, units='in', dpi = 300)
```

# 6. FDR and Power with the Presence of Moderate Variable Sampling Fractions

```{r, message=FALSE, warning=FALSE, comment=NA}
library(RColorBrewer)
## Read in original data
dat.ancom_bc=read_csv("../data/sim_fdr_power/fdr_power_ancom_bc_moderate.csv")
dat.ancom=read_csv("../data/sim_fdr_power/fdr_power_ancom_moderate.csv")
dat.deseq2=read_csv("../data/sim_fdr_power/fdr_power_deseq2_moderate.csv")
dat.edger=read_csv("../data/sim_fdr_power/fdr_power_edger_moderate.csv")
dat.zilg=read_csv("../data/sim_fdr_power/fdr_power_zilg_moderate.csv")
dat.zig=read_csv("../data/sim_fdr_power/fdr_power_zig_moderate.csv")
dat.wilcox_un=read_csv("../data/sim_fdr_power/fdr_power_wilcox_un_moderate.csv")
dat.wilcox_tss=read_csv("../data/sim_fdr_power/fdr_power_wilcox_tss_moderate.csv")

## Reshaping data
simpattern=distinct(simparams, n.taxa, n.samp.grp1, n.samp.grp2, prop.diff)

data_summary = function(eval_data, method){
  FDR=tapply(as.numeric(eval_data[1, ]), 
             rep(seq(nrow(simpattern)), each=iterNum), function(x) mean(x, na.rm = T))
  FDRSD=tapply(as.numeric(eval_data[1, ]), 
               rep(seq(nrow(simpattern)), each=iterNum), function(x) sd(x, na.rm = T))
  power=tapply(as.numeric(eval_data[2, ]), 
               rep(seq(nrow(simpattern)), each=iterNum), function(x) mean(x, na.rm = T))
  powerSD=tapply(as.numeric(eval_data[2, ]), 
                 rep(seq(nrow(simpattern)), each=iterNum), function(x) sd(x, na.rm = T))
  data_sum = data.frame(FDR, FDRSD, power, powerSD, simpattern, method)
  data_sum = data_sum%>%unite(n.samp.grp, n.samp.grp1, n.samp.grp2, sep = ", ")
  
  return(data_sum)
}

eval.dat.list = list(dat.ancom_bc, dat.ancom, dat.deseq2, dat.edger, 
                     dat.zilg, dat.zig, dat.wilcox_un, dat.wilcox_tss)
method.list = list("ANCOM-BC", "ANCOM", "DESeq2", "edgeR", "ZILG", "ZIG", "Wilcoxon", "Wilcoxon + TSS")

dat.fig.list = vector(mode = "list", length = length(eval.dat.list))
for (i in 1:length(eval.dat.list)) {
  dat.fig.list[[i]] = data_summary(eval.dat.list[[i]], method.list[[i]])
}

## Merge data
dat.fig=Reduce('rbind', dat.fig.list)
dat.fig$n.samp.grp=factor(dat.fig$n.samp.grp)
levels(dat.fig$n.samp.grp)=c("n = 20/30", "n = 50/50")
dat.fig$method=factor(dat.fig$method)
dat.fig$prop.diff=factor(dat.fig$prop.diff)
```

## 6.1 Fig. S1b

```{r, message=FALSE, warning=FALSE, comment=NA}
p=ggplot(dat.fig, aes(x=prop.diff, y=FDR, fill=method)) +
  geom_hline(yintercept=0.05, linetype="dashed", color="black", size = 0.2)+
  scale_y_continuous(breaks = c(0.05, seq(0.2, 1, 0.2)), limits = c(0, 0.8))+
  coord_flip()+facet_grid(.~n.samp.grp)+
  geom_bar(stat="identity", position=position_dodge())+
  labs(x="Proportion of Differentially Abundant Taxa", y="", fill=NULL, title="FDR")+
  scale_fill_brewer(palette="Dark2")+
  theme_bw()+
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        plot.title=element_text(hjust = 0.5),
        strip.background = element_rect(fill="white"),
        legend.position = "bottom")+
  guides(fill=guide_legend(nrow=2, byrow=TRUE))
ggarrange(p, labels = "b")
ggsave("../figures/Figure S1b.pdf", width=6.25, height=5, units='in')
ggsave("../figures/Figure S1b.jpeg", width=6.25, height=5, units='in', dpi = 300)
```

## 6.2 Fig. S1c

```{r, message=FALSE, warning=FALSE, comment=NA}
p=ggplot(dat.fig, aes(x=prop.diff, y=power, fill=method)) +
  scale_y_continuous(breaks = seq(0.2, 1, 0.2), limits = c(0, 1))+
  coord_flip()+facet_grid(.~n.samp.grp)+
  geom_bar(stat="identity", position=position_dodge())+
  labs(x="Proportion of Differentially Abundant Taxa", y="", fill=NULL, title="Power")+
  scale_fill_brewer(palette="Dark2")+
  theme_bw()+
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        strip.background = element_rect(fill="white"),
        plot.title=element_text(hjust = 0.5),
        legend.position = "bottom")+
  guides(fill=guide_legend(nrow=2, byrow=TRUE))
ggarrange(p, labels = "c")
ggsave("../figures/Figure S1c.pdf", width=6.25, height=5, units='in')
ggsave("../figures/Figure S1c.jpeg", width=6.25, height=5, units='in', dpi = 300)
```


# 7. FDR and Power with the Presence of Small Variable Sampling Fractions

```{r, message=FALSE, warning=FALSE, comment=NA}
library(RColorBrewer)
## Read in original data
dat.ancom_bc=read_csv("../data/sim_fdr_power/fdr_power_ancom_bc_small.csv")
dat.ancom=read_csv("../data/sim_fdr_power/fdr_power_ancom_small.csv")
dat.deseq2=read_csv("../data/sim_fdr_power/fdr_power_deseq2_small.csv")
dat.edger=read_csv("../data/sim_fdr_power/fdr_power_edger_small.csv")
dat.zilg=read_csv("../data/sim_fdr_power/fdr_power_zilg_small.csv")
dat.zig=read_csv("../data/sim_fdr_power/fdr_power_zig_small.csv")
dat.wilcox_un=read_csv("../data/sim_fdr_power/fdr_power_wilcox_un_small.csv")
dat.wilcox_tss=read_csv("../data/sim_fdr_power/fdr_power_wilcox_tss_small.csv")

## Reshaping data
simpattern=distinct(simparams, n.taxa, n.samp.grp1, n.samp.grp2, prop.diff)

data_summary = function(eval_data, method){
  FDR=tapply(as.numeric(eval_data[1, ]), 
             rep(seq(nrow(simpattern)), each=iterNum), function(x) mean(x, na.rm = T))
  FDRSD=tapply(as.numeric(eval_data[1, ]), 
               rep(seq(nrow(simpattern)), each=iterNum), function(x) sd(x, na.rm = T))
  power=tapply(as.numeric(eval_data[2, ]), 
               rep(seq(nrow(simpattern)), each=iterNum), function(x) mean(x, na.rm = T))
  powerSD=tapply(as.numeric(eval_data[2, ]), 
                 rep(seq(nrow(simpattern)), each=iterNum), function(x) sd(x, na.rm = T))
  data_sum = data.frame(FDR, FDRSD, power, powerSD, simpattern, method)
  data_sum = data_sum%>%unite(n.samp.grp, n.samp.grp1, n.samp.grp2, sep = ", ")
  
  return(data_sum)
}

eval.dat.list = list(dat.ancom_bc, dat.ancom, dat.deseq2, dat.edger, 
                     dat.zilg, dat.zig, dat.wilcox_un, dat.wilcox_tss)
method.list = list("ANCOM-BC", "ANCOM", "DESeq2", "edgeR", "ZILG", "ZIG", "Wilcoxon", "Wilcoxon + TSS")

dat.fig.list = vector(mode = "list", length = length(eval.dat.list))
for (i in 1:length(eval.dat.list)) {
  dat.fig.list[[i]] = data_summary(eval.dat.list[[i]], method.list[[i]])
}

## Merge data
dat.fig=Reduce('rbind', dat.fig.list)
dat.fig$n.samp.grp=factor(dat.fig$n.samp.grp)
levels(dat.fig$n.samp.grp)=c("n = 20/30", "n = 50/50")
dat.fig$method=factor(dat.fig$method)
dat.fig$prop.diff=factor(dat.fig$prop.diff)
```

## 7.1 Fig. S2b

```{r, message=FALSE, warning=FALSE, comment=NA}
p=ggplot(dat.fig, aes(x=prop.diff, y=FDR, fill=method)) +
  geom_hline(yintercept=0.05, linetype="dashed", color="black", size = 0.2)+
  scale_y_continuous(breaks = c(0.05, seq(0.2, 1, 0.2)), limits = c(0, 0.6))+
  coord_flip()+facet_grid(.~n.samp.grp)+
  geom_bar(stat="identity", position=position_dodge())+
  labs(x="Proportion of Differentially Abundant Taxa", y="", fill=NULL, title="FDR")+
  scale_fill_brewer(palette="Dark2")+
  theme_bw()+
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        plot.title=element_text(hjust = 0.5),
        strip.background = element_rect(fill="white"),
        legend.position = "bottom")+
  guides(fill=guide_legend(nrow=2, byrow=TRUE))
ggarrange(p, labels = "b")
ggsave("../figures/Figure S2b.pdf", width=6.25, height=5, units='in')
ggsave("../figures/Figure S2b.jpeg", width=6.25, height=5, units='in', dpi = 300)
```

## 7.2 Fig. S2c

```{r, message=FALSE, warning=FALSE, comment=NA}
p=ggplot(dat.fig, aes(x=prop.diff, y=power, fill=method)) +
  scale_y_continuous(breaks = seq(0.2, 1, 0.2), limits = c(0, 1))+
  coord_flip()+facet_grid(.~n.samp.grp)+
  geom_bar(stat="identity", position=position_dodge())+
  labs(x="Proportion of Differentially Abundant Taxa", y="", fill=NULL, title="Power")+
  scale_fill_brewer(palette="Dark2")+
  theme_bw()+
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(), 
        strip.background = element_rect(fill="white"),
        plot.title=element_text(hjust = 0.5),
        legend.position = "bottom")+
  guides(fill=guide_legend(nrow=2, byrow=TRUE))
ggarrange(p, labels = "c")
ggsave("../figures/Figure S2c.pdf", width=6.25, height=5, units='in')
ggsave("../figures/Figure S2c.jpeg", width=6.25, height=5, units='in', dpi = 300)
```

# Session information

```{r, message=FALSE, warning=FALSE, comment=NA}
sessionInfo()
devtools::session_info()
```
